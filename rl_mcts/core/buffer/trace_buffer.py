import numpy as np

class PrioritizedReplayBuffer():
    '''
    This class represents a replay buffer memory in which traces generated by the MCTS are stored.
    '''

    def __init__(self, max_length, task_ids, p1=0.8):
        self.task_ids = task_ids
        self.memory_task = dict((task_id, {0: [], 1: []}) for task_id in self.task_ids)
        self.stack = []
        self.max_length = max_length
        self.p1 = p1
        self.batch_length = 0

    def get_total_successful_traces(self):
        return sum([len(v[1]) for k, v in self.memory_task.items()])

    def get_total_failed_traces(self):
        return sum([len(v[0]) for k, v in self.memory_task.items()])

    def get_memory_length(self):
        return len(self.stack)

    def append_trace(self, trace):
        for tuple in trace:
            self.batch_length = len(tuple) # Set the global tuple length, it should be done only once...
            reward = 0 if tuple[4] <= 0.0 else 1
            if len(self.stack) >= self.max_length:
                t_id = self.stack[0][1]
                r = 0 if self.stack[0][4] <= 0.0 else 1
                del self.memory_task[t_id][r][0]
                del self.stack[0]
            task_id = tuple[1]
            self.memory_task[task_id][reward].append(tuple)
            self.stack.append(tuple)

    def _sample_sub_batch(self, batch_size, memory):
        indices = np.arange(len(memory))
        sampled_indices = np.random.choice(indices, size=batch_size, replace=(batch_size > len(memory)))
        batch = [[] for _ in range(self.batch_length)]
        for i in sampled_indices:
            for k in range(self.batch_length):
                batch[k].append(memory[i][k])
        return batch

    def sample_batch(self, batch_size):
        memory_0 = []
        memory_1 = []
        for task_id in self.memory_task:
            if len(self.memory_task[task_id][1]) > 0:
                memory_0 += self.memory_task[task_id][0]
                memory_1 += self.memory_task[task_id][1]

        if len(memory_0) == 0 and len(memory_1) == 0:
            return None
        elif len(memory_1) > 0 and len(memory_0) == 0:
            batch = self._sample_sub_batch(batch_size, memory_1)
        elif len(memory_0) > 0 and len(memory_1) == 0:
            batch = self._sample_sub_batch(batch_size, memory_0)
        else:
            buffer_binomial_distrib = np.random.binomial(1, self.p1, batch_size)
            sub_batch_r1_size = sum(buffer_binomial_distrib)
            sub_batch_r0_size = batch_size - sub_batch_r1_size
            assert sub_batch_r1_size+sub_batch_r0_size == batch_size, 'problem with batch sizes!'
            batch = self._sample_sub_batch(sub_batch_r1_size, memory_1)
            batch += self._sample_sub_batch(sub_batch_r0_size, memory_0)

        return batch if batch else None

    def empty_memory(self):
        self.memory_task = dict((task_id, {0: [], 1: []}) for task_id in self.task_ids)
        self.stack = []